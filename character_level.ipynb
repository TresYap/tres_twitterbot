{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level modelling implementation where model tries to predict the next character in a tweet\n",
    "\n",
    "### Challenge to self: Convert implementation to LSTM to learn how to build models urself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mobius strips are overrated theyre just bracel...</td>\n",
       "      <td>2020-08-03 04:07:12+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>when is the next long weekend</td>\n",
       "      <td>2020-08-03 02:50:52+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>just realized that parasite's ramdon is a case...</td>\n",
       "      <td>2020-07-30 04:16:08+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>We are actually fucked lmao</td>\n",
       "      <td>2020-07-30 03:55:00+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>To this day i still have no idea when i should...</td>\n",
       "      <td>2020-07-30 03:48:42+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  mobius strips are overrated theyre just bracel...   \n",
       "1           1                      when is the next long weekend   \n",
       "2           2  just realized that parasite's ramdon is a case...   \n",
       "3           3                        We are actually fucked lmao   \n",
       "4           4  To this day i still have no idea when i should...   \n",
       "\n",
       "                        date  favorites  retweets hashtags  \n",
       "0  2020-08-03 04:07:12+00:00          5         1      NaN  \n",
       "1  2020-08-03 02:50:52+00:00         10         0      NaN  \n",
       "2  2020-07-30 04:16:08+00:00          6         1      NaN  \n",
       "3  2020-07-30 03:55:00+00:00          6         1      NaN  \n",
       "4  2020-07-30 03:48:42+00:00          2         0      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mobius strips are overrated theyre just bracel...</td>\n",
       "      <td>2020-08-03 04:07:12+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>when is the next long weekend</td>\n",
       "      <td>2020-08-03 02:50:52+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>just realized that parasite's ramdon is a case...</td>\n",
       "      <td>2020-07-30 04:16:08+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>We are actually fucked lmao</td>\n",
       "      <td>2020-07-30 03:55:00+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>To this day i still have no idea when i should...</td>\n",
       "      <td>2020-07-30 03:48:42+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>983</td>\n",
       "      <td>Applies to the advice i give as well</td>\n",
       "      <td>2018-01-01 12:54:14+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>984</td>\n",
       "      <td>Jan 5-6 ah</td>\n",
       "      <td>2018-01-01 12:24:09+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>985</td>\n",
       "      <td>What was your favorite anime of 2017 and why i...</td>\n",
       "      <td>2018-01-01 05:38:32+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>986</td>\n",
       "      <td>M R . W O R L D W I D E</td>\n",
       "      <td>2018-01-01 03:32:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>New Year New Me jk cant wait to beg for soldie...</td>\n",
       "      <td>2018-01-01 02:18:09+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               text  \\\n",
       "0             0  mobius strips are overrated theyre just bracel...   \n",
       "1             1                      when is the next long weekend   \n",
       "2             2  just realized that parasite's ramdon is a case...   \n",
       "3             3                        We are actually fucked lmao   \n",
       "4             4  To this day i still have no idea when i should...   \n",
       "..          ...                                                ...   \n",
       "983         983               Applies to the advice i give as well   \n",
       "984         984                                         Jan 5-6 ah   \n",
       "985         985  What was your favorite anime of 2017 and why i...   \n",
       "986         986                            M R . W O R L D W I D E   \n",
       "987         987  New Year New Me jk cant wait to beg for soldie...   \n",
       "\n",
       "                          date  favorites  retweets hashtags  \n",
       "0    2020-08-03 04:07:12+00:00          5         1      NaN  \n",
       "1    2020-08-03 02:50:52+00:00         10         0      NaN  \n",
       "2    2020-07-30 04:16:08+00:00          6         1      NaN  \n",
       "3    2020-07-30 03:55:00+00:00          6         1      NaN  \n",
       "4    2020-07-30 03:48:42+00:00          2         0      NaN  \n",
       "..                         ...        ...       ...      ...  \n",
       "983  2018-01-01 12:54:14+00:00          0         0      NaN  \n",
       "984  2018-01-01 12:24:09+00:00          3         0      NaN  \n",
       "985  2018-01-01 05:38:32+00:00          0         0      NaN  \n",
       "986  2018-01-01 03:32:46+00:00          0         0      NaN  \n",
       "987  2018-01-01 02:18:09+00:00          2         0      NaN  \n",
       "\n",
       "[988 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    988.000000\n",
       "mean      78.675101\n",
       "std       47.504095\n",
       "min        2.000000\n",
       "25%       42.000000\n",
       "50%       71.000000\n",
       "75%      105.250000\n",
       "max      279.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.text.str.cat(sep=\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobius strips are overrated theyre just bracelets that u didnt put on correctly~when is the next long weekend~just realized that parasite's ramdon is a case study for barthes' steak and chips thought experiment and now i want to kill myself for thinking abt barthes for fun~We are\n"
     ]
    }
   ],
   "source": [
    "print(text[:280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing characters\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mobius strips' ---- characters mapped to int ---- > [72 74 61 68 80 78  0 78 79 77 68 75 78]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(72, shape=(), dtype=int32) :  m\n",
      "tf.Tensor(74, shape=(), dtype=int32) :  o\n",
      "tf.Tensor(61, shape=(), dtype=int32) :  b\n",
      "tf.Tensor(68, shape=(), dtype=int32) :  i\n",
      "tf.Tensor(80, shape=(), dtype=int32) :  u\n"
     ]
    }
   ],
   "source": [
    "#Length of a single input\n",
    "seq_length = 20\n",
    "examples_per_epoch = len(text)\n",
    "\n",
    "#convert to tensor\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(i, \": \", idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mobius strips are ove'\n",
      "'rrated theyre just br'\n",
      "'acelets that u didnt '\n",
      "'put on correctly~when'\n",
      "' is the next long wee'\n"
     ]
    }
   ],
   "source": [
    "#Make batches of seq_length\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate and shift (make train and target)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tresyap\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 20 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c02df3153a0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Given [0:99], predict [1:100]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_example\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_example\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_example\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_example\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 20 were indexed"
     ]
    }
   ],
   "source": [
    "#Given [0:99], predict [1:100] \n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(repr(''.join(idx2char[input_example])))\n",
    "    print(repr(''.join(idx2char[target_example])), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 72 ('m')\n",
      "  expected output: 74 ('o')\n",
      "Step    1\n",
      "  input: 74 ('o')\n",
      "  expected output: 61 ('b')\n",
      "Step    2\n",
      "  input: 61 ('b')\n",
      "  expected output: 68 ('i')\n",
      "Step    3\n",
      "  input: 68 ('i')\n",
      "  expected output: 80 ('u')\n",
      "Step    4\n",
      "  input: 80 ('u')\n",
      "  expected output: 78 ('s')\n"
     ]
    }
   ],
   "source": [
    "#visualization of training task for each rnn unit\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000 #buffer size is for memory i think\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 20), (64, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#64 training examples of seq length 100\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the thing (Personal task: change to LSTM to learn how to build ur own )\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        \n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, dropout=0.5),\n",
    "        \n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, dropout=0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(vocab_size) \n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 87)\n"
     ]
    }
   ],
   "source": [
    "# 64 batch size, 100 seq length per batch, 86 vocab size (unique characters/tokens)\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22272     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 87)            44631     \n",
      "=================================================================\n",
      "Total params: 3,741,015\n",
      "Trainable params: 3,741,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "With the setup of RNN, text generation will be treated like a basic supervised classification problem (given char x, predict char y), meaning we can use standard backprop rules and loss functions (cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 20, 87)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.466755\n"
     ]
    }
   ],
   "source": [
    "#mean loss for the batch of 64 is 4.45\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving checkpoints (remember this lol)\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'D:/garbage_models/twitterbot'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "#saves weights in directory\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model (adam optimizer because why not, loss = loss function)\n",
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900\n",
      "Train for 58 steps\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5578\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5563\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 0.5630\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5591\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5591\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5622\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5578\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5575\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5557\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5559\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5526\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5589\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5544\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5589\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5573\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5508\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5580\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5574\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5526\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5548\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5558\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5526\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5593\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5548\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5603\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5627\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5568\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5562\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5568\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5583\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5591\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5529\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5615\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5619\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5577\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5549\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5607\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5622\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5568\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5580\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5561\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5593\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5578\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5528\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5616\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5530\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5557\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5549\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5583\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5572\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5565\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5600\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5564\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5615\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5571\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5591\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5524\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5584\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5565\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5577\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5616\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5515\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5545\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5541\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5575\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5567\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5569\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5532\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5569\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5584\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5600\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5549\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5546\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5569\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5581\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5546\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5513\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5576\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5561\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5626\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5559\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5517\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5580\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5582\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5551\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5530\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5619\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5599\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5523\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5533\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5539\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5551\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5567\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5556\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5582\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5564\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5632\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5520\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5562\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 0.5591\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "epochs = 100\n",
    "#train the model (training set, number of epochs, callback for saving checkpoints)\n",
    "for i in range (0, 20):\n",
    "    clear_output()\n",
    "    print(i*100)\n",
    "    history = model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction step will use batch size 1 because rnns take in input by a fixed batch size\n",
    "# so its necessary to run it with a different batch size of 1 since our task is to predict the next character\n",
    "# tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.save_weights('./textbot/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import weights to model \n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./textbot/model')\n",
    "model.build(tf.TensorShape([1, None])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22272     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 512)            1574912   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 87)             44631     \n",
      "=================================================================\n",
      "Total params: 3,741,015\n",
      "Trainable params: 3,741,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Loop\n",
    "Set starting string, <br>\n",
    "Get next character using prediction distribution, <br>\n",
    "Next character is used as input, <br>\n",
    "repeat until size reached <br>\n",
    "<br>\n",
    "note that because an rnn has more info, the state changes and gets context from previous timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    #number of characters to generate\n",
    "    num_generate = 300\n",
    "    \n",
    "    #vectorize start string to number\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) #converts to tensor batch i think? \n",
    "    \n",
    "    #results\n",
    "    output = []\n",
    "    \n",
    "    #Adjust temperature for predictability (higher for more surprising text (idk either))\n",
    "    temperature = 0.25\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        #remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        #next character from categorical distribution\n",
    "        predictions = predictions/temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        #preedicted character = next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        output.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~i get turned on by jojo meme pages back hurts again so i dont get regretation of a philosophy does not compel is a joke~i stay it doesnt know that theres of something for work~I was just mikiding a hiphop version of this month~how the whole day later i still dont notell us what the game just john le'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p;fbclid=IwAR2Pdh8Bk proves that the way sora wears ago but fuck its a problem of my attic regression this here to things on my best for saying the same pitfalls of joke i last 5 minutes after und more time in school~Plato string any progress with ur own mouse~Ppl are laughing about machine learning '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cause its a very good so ur gonna hear a lot of landmare is being able to afford in my see god is this wakanist all my activity is the rest of the word fast enough to thinks my cynicism associated w blood really bad~This my face with shitposts >*st i can say i'm unemployed wtf~ime to be excited about\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"t3 >realized that i didnt realize how course im not above all the devil works harder.~I wasnt talking about machine learning is easy just take the same way shinji's the logical equivat a universe that exhis. I hope you'll be make a Transformers are the tequila CODE, we love working with so many people\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "generate_text(model, idx2char[random.randint(0,86)] + idx2char[random.randint(0,86)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Tensorflow",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
